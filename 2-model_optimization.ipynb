{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataprocessing import generate_tasks\n",
    "from fitting import get_model, fit\n",
    "from meta import Meta\n",
    "import optuna as opt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(trial, train_signals, val_signals, bkg_file, args):\n",
    "    # Manually seed torch and numpy for reproducible results\n",
    "    torch.manual_seed(args[\"seed\"])\n",
    "    torch.cuda.manual_seed_all(args[\"seed\"])\n",
    "    np.random.seed(args[\"seed\"])\n",
    "    \n",
    "    # Defining trial parameters\n",
    "    sup_shots = trial.suggest_int(\"k_sup\", 100, 100)\n",
    "    que_shots = trial.suggest_int(\"k_que\", 200, 200)\n",
    "    num_layers = trial.suggest_int(\"num_hidden_layers\", 1, 4)\n",
    "    hidden_layers = []\n",
    "    for i in range(num_layers):\n",
    "        num_features = trial.suggest_int(f\"num_features_layer_{i}\", 20, 150)\n",
    "        hidden_layers.append(num_features)\n",
    "\n",
    "    # Generate tasks from the signal\n",
    "    args[\"k_sup\"], args[\"k_que\"] = sup_shots, que_shots\n",
    "    train_tasks = generate_tasks(train_signals, bkg_file, sup_shots, que_shots)\n",
    "    val_tasks = generate_tasks(val_signals, bkg_file, sup_shots, que_shots)\n",
    "\n",
    "    # Choose PyTorch device and create the model\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model = Meta(args, get_model(hidden_layers), device).to(device)\n",
    "    \n",
    "    # Setup Weights and Biases logger, config hyperparams and watch model\n",
    "    wandb.init(project=\"Meta-HEP\")\n",
    "    name = f\"K{sup_shots}Q{que_shots}-HL{num_layers}-\"\n",
    "    for i in range(num_layers):\n",
    "        features = hidden_layers[i]\n",
    "        name += f\"F{features}\"\n",
    "    wandb.run.name = name\n",
    "    wandb.config.update(args)\n",
    "    wandb.watch(model)\n",
    "    print(f\"RUN NAME: {name}\")\n",
    "\n",
    "    # Fit the model and return best loss\n",
    "    return fit(model, train_tasks, val_tasks, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define optimization parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define study arguments\n",
    "args = {\n",
    "    \"epochs\": 1000,\n",
    "    \"epoch_steps\": 1000,\n",
    "    \"eval_steps\": 200,\n",
    "    \"patience\": 12,\n",
    "    \"meta_lr\": 1e-3,\n",
    "    \"update_lr\": 1e-2,\n",
    "    \"lr_type\": \"vector\",\n",
    "    \"seed\": 42\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather the tasks to run the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datapath and background file\n",
    "datapath = \"processed-data/\"\n",
    "bkg_file = datapath + \"bkg.h5\"\n",
    "\n",
    "# Signal files for each split\n",
    "train_signals = [\"hg3000_hq1000\", \"hg3000_hq1400\", \"wohg_hq1200\"]\n",
    "val_signals = [\"hg3000_hq1200\", \"wohg_hq1000\"]\n",
    "test_signals = [\"wohg_hq1400\", \"fcnc\"]\n",
    "\n",
    "# Add datapath and extention to files for each split\n",
    "train_signals = [datapath + p + \".h5\" for p in train_signals]\n",
    "val_signals = [datapath + p + \".h5\" for p in val_signals]\n",
    "test_signals = [datapath + p + \".h5\" for p in test_signals]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create study and find optimum model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = \"Fixed K-support and K-query optimization\"\n",
    "study = opt.create_study(study_name=study_name, storage='sqlite:///meta-model.db', load_if_exists=True)\n",
    "objective = lambda trial: optimize(trial, train_signals, val_signals, bkg_file, args)\n",
    "study.optimize(objective, n_trials=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "89f2b142862c6413ccfdaa721ec63414551642d13586bc42d72017c6bf8faafa"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
