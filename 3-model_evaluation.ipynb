{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meta import get_class_weights, Meta, to_numpy\n",
    "from fitting import get_model, get_model_name\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from dataprocessing import generate_tasks\n",
    "from torch.nn import functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_model(filepath):\n",
    "    \n",
    "    # Model architecture\n",
    "    filename = filepath.split(\"/\")[-1]\n",
    "    name = filename[:-3]\n",
    "    dropout = float(name[name.find(\"D\")+len(\"D\"):name.find(\"D\")+5])\n",
    "    sup_shots = int(name[name.find(\"K\")+len(\"K\"):name.rfind(\"Q\")])\n",
    "    que_shots = int(name[name.find(\"Q\")+len(\"Q\"):name.rfind(\"-H\")])\n",
    "    hidden_layers = name.split(\"F\")[1:]\n",
    "    hidden_layers = [int(hl) for hl in hidden_layers]\n",
    "\n",
    "    # Arguments to create model\n",
    "    config = get_model(hidden_layers, dropout)\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # Choose PyTorch device and load the model\n",
    "    model = Meta(name, config, sup_shots, que_shots, device).to(device)\n",
    "    model.load(filepath)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datapath and background file for data-files\n",
    "datapath = \"processed-data/\"\n",
    "bkg_file = datapath + \"bkg.h5\"\n",
    "\n",
    "# Signal files for each task split\n",
    "val_signals = [\"hg3000_hq1200\", \"wohg_hq1000\"]\n",
    "test_signals = [\"wohg_hq1400\", \"fcnc\"]\n",
    "\n",
    "# Add datapath and extention to files for each split\n",
    "val_signals = [datapath + p + \".h5\" for p in val_signals]\n",
    "test_signals = [datapath + p + \".h5\" for p in test_signals]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on the val and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(model, sup_task, que_task, total_steps):\n",
    "    # Get total steps\n",
    "    # total_steps = len(list(que_task[\"data\"]))\n",
    "    \n",
    "    # Iterate the whole query dataloader\n",
    "    roc_steps = 0\n",
    "    metrics = {\"roc\": 0, \"acc\": 0, \"loss\": 0}\n",
    "    for _ in tqdm(range(total_steps), total=total_steps, desc=\"Iterating task loader\", leave=False):\n",
    "        \n",
    "        # Get support and query samples from task\n",
    "        x_sup, w_sup, y_sup = next(sup_task[\"data\"])\n",
    "        sup_weights = get_class_weights(sup_task[\"weights\"], y_sup)\n",
    "        x_que, w_que, y_que = next(que_task[\"data\"])\n",
    "        que_weights = get_class_weights(que_task[\"weights\"], y_que)\n",
    "\n",
    "        # Normalize weights\n",
    "        w_sup = w_sup / w_sup.sum() * w_sup.shape[0]\n",
    "        w_que = w_que / w_que.sum() * w_que.shape[0]\n",
    "        \n",
    "        # Model prediction\n",
    "        y_hat = model.predict(x_sup, w_sup, y_sup, x_que, sup_weights)\n",
    "        \n",
    "        # Get roc (try except if only one class is present in sample)\n",
    "        try:\n",
    "            metrics[\"roc\"] += roc_auc_score(to_numpy(y_que), to_numpy(y_hat.round()), sample_weight=to_numpy(w_que))\n",
    "            roc_steps += 1\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Get accuracy and loss\n",
    "        metrics[\"acc\"] += (torch.eq(y_hat.round(), y_que) * w_que).sum().item() / y_hat.shape[0]\n",
    "        metrics[\"loss\"] += (F.binary_cross_entropy(y_hat, y_que, reduction=\"none\") * w_que * que_weights).mean().item()\n",
    "        \n",
    "    # Average the metrics\n",
    "    metrics[\"acc\"] /= total_steps\n",
    "    metrics[\"loss\"] /= total_steps\n",
    "    metrics[\"roc\"] /= roc_steps    \n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56c13b97826405384305159c7088356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterating models:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39701b4726542ed859b2095c859c4bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterating number of shots:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Populating tasks:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb4da9730544e079a7e221a405f6c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterating Tasks:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterating task loader:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "942cbd4a9104493f89707f2ba85a8b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterating task loader:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Defining arguments for metric retrieval\n",
    "sup_shots_list = [5, 10, 25, 50, 100, 250, 500]\n",
    "que_shots_list = [2 * sup_shots for sup_shots in sup_shots_list]\n",
    "total_samples = 50000\n",
    "\n",
    "# Initializations\n",
    "metrics = {}\n",
    "model_paths = list(glob.glob(\"models/*.pt\"))\n",
    "shots_list = zip(sup_shots_list, que_shots_list)\n",
    "\n",
    "# Iterate all models\n",
    "for model_path in tqdm(model_paths, total=len(model_paths), desc=\"Iterating models\"):\n",
    "\n",
    "    # Fetch model\n",
    "    model = fetch_model(model_path)\n",
    "    model_name = f\"{model.k_sup}-{model.k_que}\"\n",
    "    \n",
    "    # Add entry to metrics dict\n",
    "    metrics[model_name] = {}\n",
    "    \n",
    "    # Iterate all number of support and query shots to evaluate from\n",
    "    for sup_shots, que_shots in tqdm(shots_list, total=len(sup_shots_list), desc=\"Iterating number of shots\", leave=False):\n",
    "\n",
    "        # Generate tasks\n",
    "        # val_tasks = generate_tasks(val_signals, bkg_file, sup_shots, que_shots)\n",
    "        test_tasks = generate_tasks(test_signals, bkg_file, sup_shots, que_shots)\n",
    "\n",
    "        # Add entry to metrics\n",
    "        shot_name = f\"{sup_shots}-{que_shots}\"\n",
    "        metrics[model_name][shot_name] = {}\n",
    "\n",
    "        # Iterate all test signals\n",
    "        for signal in tqdm(test_tasks, total=len(test_tasks), desc=\"Iterating Tasks\", leave=False):\n",
    "\n",
    "            # Get support sample\n",
    "            sup_task, que_task = test_tasks[signal][\"sup\"], test_tasks[signal][\"que\"]\n",
    "            x_sup, w_sup, y_sup = next(sup_task[\"data\"])\n",
    "            sup_weights = get_class_weights(sup_task[\"weights\"], y_sup)\n",
    "\n",
    "            # Get metrics for this models, shot and task\n",
    "            total_steps = total_samples // sup_shots\n",
    "            metrics[model_name][shot_name][signal] = get_metrics(model, sup_task, que_task, total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f5ff148e73835ca4fa28de0f0a60b6e4be9e22106b47c94548bad0fe3aa25f7"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
